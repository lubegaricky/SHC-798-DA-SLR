---
title: "Data Analysis & Simple Linear Regression"
author: "Richard Lubega"
date: "`r Sys.Date()`"
output: html_document
pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 1

## Part 1: Data Analysis with R

```{r, Part-1-a_b}
# Getting Started with the Dataset:
cat("\n=== Getting Started with the Dataset ===\n")
pacman::p_load(ggplot2) # checks if ggplot2 is installed;if it's not installed, it automatically installs it first, then loads it. If it's already installed, it just loads it into the current R session
pacman::p_load(tidymodels) # tidymodels some useful packages and functionalities

cat("\n=== View first few rows of the dataset ===\n")
head(mpg) # View first few rows of the dataset

cat("\n=== Get an overview of the dataset ===\n")
summary(mpg) # Get an overview of the dataset

# --- Add ----
str(mpg)
# view(mpg)
# -------------

#Analyse the mpg dataset using descriptive methods

#(a)
# average city and highway fuel economy across all vehicle classes
cat("\n=== Average city and highway fuel economy, afe, across all vehicle classes ===\n")
afe <- aggregate(cbind(cty, hwy) ~ class, data = mpg, FUN = mean)
afe

#(b)
# Compare the fuel efficiency (cty and hwy)
cat("\n=== Comparing fuel efficiency for cty and hwy economies ===\n")
par(mfrow = c(1, 2)) # Set up a 1x2 plot layout for side-by-side boxplots

# Boxplot for city mpg by cylinders
boxplot(cty ~ cyl, data = mpg, 
        main = "City mpg by Number of Cylinders", 
        xlab = "Cylinders", 
        ylab = "City mpg (miles per gallon)")

# Boxplot for highway mpg by cylinders
boxplot(hwy ~ cyl, data = mpg, 
        main = "Highway mpg by Number of Cylinders", 
        xlab = "Cylinders", 
        ylab = "Highway mpg (miles per gallon)")

par(mfrow = c(1, 1)) # Reset plot layout to default

# Combine plots by faceting
cat("\n=== Combining the box plots for comparison ===\n")
mpg_comb <- mpg %>%
  select(cyl, cty, hwy) %>%
  pivot_longer(cols = c(cty, hwy), names_to = "fuel_econ", values_to = "mpg")

ggplot(mpg_comb, aes(x = factor(cyl), y = mpg, fill = fuel_econ)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Fuel Efficiency by Number of Cylinders",
       x = "Number of Cylinders",
       y = "miles per gallon",
       fill = "Fuel Economy") +
  scale_fill_manual(values = c("cty" = "lightblue", "hwy" = "lightcoral"),
                    labels = c("City", "Highway")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

#---------  Median Values by cylinder count -------------
cat("\n=== Median Values by cylinder count ===\n")
mpg %>%
  group_by(cyl) %>%
  summarise(
    median_cty = median(cty),
    median_hwy = median(hwy),
    .groups = 'drop'
  )

cat("\n=== Trend Analysis ===\n")
```

#### **Commenting on the Trend**

This analysis clearly demonstrates that engine size (cylinder count) is a major predictor of fuel efficiency, with smaller engines being visibly more fuel-efficient than larger ones. Some *outliers* exist (may be due to high-efficiency hybrids or low-efficiency compact cars).

-   **Inverse relationship:** Based on the boxplots (where more cylinders = lower mpg), there's a clear *negative* correlation between the number of cylinders and fuel efficiency (mpg). As cylinder count increases, both city and highway mpg decrease.
-   **Highway vs City efficiency:** Highway mpg is consistently higher than city mpg across all cylinder counts (as seen from the combined plot), which may be explained by the more efficient cruising speeds on highways. Generally, the **fuel efficiency difference** between city and highway driving becomes more pronounced in vehicles with fewer cylinders.
-   **4-cylinder cars** are the most fuel-efficient, with median values of 21 mpg (for city) and 29 mpg (for highway). The rest in each category have lower values. **8-cylinder cars** are the least fuel-efficient, with median values of 13 mpg ( for city) and 17 mpg (for highway).
-   **5-cylinder cars** are the least common (narrower range) in both categories. This may be due to fewer models of these cars. **6-cylinder cars** have the most broad range compared to the others
-   There is also **variability within cylinder** groups, and is most pronounced in **6-cylinder cars**, whch suggests that factors beyond cylinder count (including vehicle weight, engine technology, etc.) also influence fuel efficiency.

```{r, Part-1-c}
# (c)
# Correlation: Engine Displacement vs Highway Fuel Economy
cat("\nCorrelation: engine displacement (displ) and highway fuel economy (hwy) \n") 

# Calculate correlation coefficient
correlation_pearson <- cor(mpg$displ, mpg$hwy)
correlation_spearman <- cor(mpg$displ, mpg$hwy, method = "spearman")

cat("Pearson correlation coefficient:", round(correlation_pearson, 4), "\n")
cat("Spearman correlation coefficient:", round(correlation_spearman, 4), "\n")

# Interpretation of correlation strength
interpret_correlation <- function(r) {
  abs_r <- abs(r)
  if (abs_r >= 0.7) return("Strong")
  else if (abs_r >= 0.3) return("Moderate")
  else return("Weak")
}

cat("Correlation strength:", interpret_correlation(correlation_pearson), "\n")
cat("Direction:", ifelse(correlation_pearson > 0, "Positive", "Negative"), "\n")

# Create basic scatter plot
cat("\n=== Creating a Basic Scatter Plot ===\n")

# Basic scatter plot
plot_dh <- ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point(alpha = 0.9, size = 2, color = "black") +
    labs(
    title = "Engine Displacement vs Highway Fuel Economy",
    subtitle = paste("Pearson r =", round(correlation_pearson, 3)),
    x = "Engine Displacement (l)",
    y = "Highway Fuel Economy (mpg)",
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    axis.title = element_text(size = 11),
    panel.grid.minor = element_blank()
  )

print(plot_dh)

cat("\nTest the significance of the correlation \n")
cor_test <- cor.test(mpg$displ, mpg$hwy, method = "pearson")
cat("Pearson correlation test:\n")
print(cor_test)
cat("\nSignificance level: ", ifelse(cor_test$p.value < 0.001, "p < 0.001 (highly significant)", 
                                   ifelse(cor_test$p.value < 0.01, "p < 0.01 (significant)", 
                                          ifelse(cor_test$p.value < 0.05, "p < 0.05 (significant)", "not significant"))), "\n")

```

Therefore, based on the analysis, a **strong negative**, **highly** statistically **significant** correlation exists between engine displacement (displ) and highway fuel economy (hwy).

The scatter plot reinforces this because as displacement increases, highway mpg decreases.

```{r, Part-1-d}
# Linear regression
cat("\n Linear Regression Model \n")
lm_model <- lm(hwy ~ displ, data = mpg)
summary(lm_model)
plot(mpg$displ, mpg$hwy)+
  abline(lm_model, col = "red")

cat("\n Model Diagnostics \n")
# Diagnostics plots
par(mfrow = c(2,2))
plot(lm_model)

par(mfrow = c(1,1))
# Tukey-Anscombe Plot
# plot(lm_model$fitted.values, lm_model$residuals, xlab="Fitted", ylab="Residuals", pch=20) +
#   title("Residuals vs. Fitted Values") +
#   lines(loess.smooth(lm_model$fitted.values, lm_model$residuals),col="red") +
#   abline(h=0, col="grey")

# Residuals vs. Predictor Plot
plot(mpg$displ, lm_model$residuals, xlab="predictor (displ)", ylab="Residuals", pch=20) +
  title("Residuals vs. Predictor displ") +
  lines(loess.smooth(mpg$displ, lm_model$residuals),col="red") +
  abline(h=0, col="grey")

# Quantile-Quantile Plot
qqnorm(lm_model$residuals) #Quantile-Quantile Plot
qqline(lm_model$residuals) # adds the diagonal line

```

From the **model diagnostics (Tuskey-Anscombe plot)**, the red LOESS line is slightly curved which indicates non-linearity. Nonetheless, the expectation of the residuals can be considered zero. The variance of the errors increases with fitted values and homoskedasticity is violated.

The **Q-Q plot** indicates that the bulk of the residuals (in the central region) are approximately Gaussian distributed. The data exhibits heavy tails (skewness) and has outliers at the extremes. The noticeable presence of extreme positive residuals suggests a right-skewed distribution (departure from normality), the assumption of Gaussian errors is violated by the model.

To improve the model, variable transformation is required (to stabilize the spread and ensure error normality).

#### **Comment on Model Outputs**:

The regression model, **lm_model** predicts highway fuel economy (hwy, in mpg) as a function of engine displacement (displ, in litres).

-   **Regression Coefficients**:

    -   **Intercept** (35.6977) implies that when engine displacement is theoretically 0 litres, the predicted highway fuel economy is approximately 35.7 mpg. It's p-value (\< 2e-16) is very small and indicates that it is statistically significant.
    -   **Slope** (-3.5306): For each 1-litre increase in engine displacement, highway fuel economy decreases by approximately 3.53 mpg, on average. The t-value (-15.07) and p-value (\< 2e-16) indicate this coefficient is highly significant, confirming a strong negative relationship.

-   **Statistical Significance**:

The p-value for displ is very small (\< 2.2e-16), meaning the relationship is statistically significant. Engine size is a strong/meaningful predictor of fuel efficiency.

-   **Model Fit**:

    -   The **multiple R-squared** (0.5868) and **adjusted R-squared** (0.585) indicate that approximately 58.68% of the variability in highway fuel economy is explained by engine displacement. This suggests a moderately strong negative relationship, but other factors (e.g., vehicle weight, transmission type) may also play a role.

#### **Implication of Model Outputs on the Relationship**

-   The **negative coefficient** for *displ* (-3.5306) supports the belief that cars with smaller engines have better fuel efficiency. As engine size increases, highway fuel economy decreases significantly, with a 1-liter increase in displacement leading to a 3.53 mpg reduction in fuel efficiency, on average. The highly significant **p-values** for both the displ coefficient and the overall model (\< 2e-16) confirm that the negative relationship between engine size and fuel efficiency is *not due to random chance*. This strengthens the conclusion that engine size is a reliable predictor of fuel efficiency.
-   The **R-squared value** (0.5868) indicates that engine size alone doesn't explain all the variability in fuel efficiency, so the remaining 41.32% of variability implies other factors (like, vehicle weight, car transmission type, or car drive type) also influence fuel efficiency.

## Part 2: Data Smoothing

Traffic Flow Data Analysis

```{r, Smoothing}
# Traffic flow data
hour <- 6:18
vehicles <- c(200, 350, 500, 420, 380, 300, 250, 220, 200, 280, 400, 550, 600)
traffic <- data.frame(hour, vehicles)

#(a)
running_mean <- sapply(2:(length(vehicles)-1), function(i) mean(vehicles[(i-1):(i+1)]))
hours_smooth <- hour[2:(length(hour)-1)]
plot(hour, vehicles, type = "p", main = "Running Mean (Manual)", xlab = "Hour", ylab = "Vehicles")
lines(hours_smooth, running_mean, type = "l", col = "blue")

#(b)
ks <- ksmooth(hour, vehicles, kernel = "box", bandwidth = 1)
plot(hour, vehicles, main = "ksmooth (Box kernel)", xlab = "Hour", ylab = "Vehicles")
lines(ks, col = "blue")

#(c)
ks_gauss <- ksmooth(hour, vehicles, kernel = "normal", bandwidth = 2)
plot(hour, vehicles, main = "ksmooth (Gaussian kernel)", xlab = "Hour", ylab = "Vehicles")
lines(ks_gauss, col = "blue")

#(d)
ks_gauss2 <- ksmooth(hour, vehicles, kernel = "normal", bandwidth = 1)
lines(ks_gauss2, col = "red") # Compare with previous

#(e)
lo <- loess(vehicles ~ hour, data = traffic, span = 0.3, degree = 2)
hour_seq <- seq(6, 18, 0.1)
lo_pred <- predict(lo, newdata = data.frame(hour = hour_seq))
plot(hour, vehicles, main = "LOESS Smoother", xlab = "Hour", ylab = "Vehicles")
lines(hour_seq, lo_pred, col = "purple")

```

## Part 3: Simple regression

### Question 1

```{r, Part-3-1}
# The dataset cars
# A SLR to analyse the relationship between speed and stopping distance

cat("\n A SLR between speed and stopping distance \n")
lm_s.sd <- lm(dist ~ speed, data = cars)
summary(lm_s.sd)

cat("\n ===  SLR Model Plot  === \n")
plot(cars$speed, cars$dist)
abline(lm_s.sd, col = "blue")

```

#### (a)

From the model summary, **Multiple R-squared**: 0.6511, Adjusted R-squared: 0.6438

Thus, **65.11%** of the variation in stopping distance is explained by speed

#### (b)

**Intercept** (-17.5791): This means that for a theoretical speed of 0 mph the predicted stopping distance is -17.5791 feet. This is not practically rational but ensures the regression line fits the data best within the observed speed range. It is not meaningful to extrapolate to speed = 0.

-   It's p-value (0.0123) is small and statistically significant at the 5% level, but its practical importance is limited.

**Slope** (3.9324): For every 1 mph increase in speed, stopping distance increases by about 3.9324 feet. Higher driving speeds require longer stopping distances.

-   The p-value (1.49e-12) is much smaller than 0.05 (even at a 1% significance level), so the relationship between speed and stopping distance is statistically significant.We reject the null hypothesis that speed has no effect on stopping distance. Thus, speed has an considerable impact on stopping distance.

#### (c)

```{r, Part-3-1-c}
# Predicting stopping distance for  speed = 20 mph;  compute a 95% prediction interval.
cat("\n ===  Stopping distance at a speed of 20 mp and the 95% prediction interval ===\n ")
predict(lm_s.sd, newdata = data.frame(speed = 20), interval = "prediction", level = 0.95)

```

#### (d)

```{r, Part-3-1-d}
cat("\n Evaluating Model Assumptions \n")
cat("\n ===  Model Diagnostics Plots  === \n")
# Diagnostics plots
par(mfrow = c(2,2))
plot(lm_s.sd)

par(mfrow = c(1,1))
# Tukey-Anscombe Plot
plot(lm_s.sd$fitted.values, lm_s.sd$residuals, xlab="Fitted", ylab="Residuals", pch=20) +
  title("Residuals vs. Fitted Values") +
  lines(loess.smooth(lm_s.sd$fitted.values, lm_s.sd$residuals),col="red") +
  abline(h=0, col="grey")

# Residuals vs. Predictor Plot
plot(cars$speed, lm_s.sd$residuals, xlab="predictor (speed)", ylab="Residuals", pch=20) +
  title("Residuals vs. Predictor displ") +
  lines(loess.smooth(cars$speed, lm_s.sd$residuals),col="red") +
  abline(h=0, col="grey")

# Quantile-Quantile Plot
qqnorm(lm_s.sd$residuals) #Quantile-Quantile Plot
qqline(lm_s.sd$residuals) # adds the diagonal line
```

#### **Model Assumption Evaluation**

1.  **Linearity** — *From the Tukey-Anscombe Plot (Residuals vs. Fitted)*:

-   By inspection, the residuals generally hover around the zero line which suggests that they likely approximate a mean of zero. There is, however, slight curvature (a kink) in the red LOESS smoother line (deviation from the horizontal) which implies mild (misspecified) non-linearity. This is confirmed by the systematic misprediction in the middle (overpredicting) and the extremes (underpredicting). In this case, there is is a clear violation of the linearity (E[E~*i*~] = 0 ) assumption; a straight line is not the correct fit to the data and the model ought to be improved.

-   **Transformation**: Add a quadratic term (dist = β~0~ + β~1~ . speed + β~2~ . speed^2^ + E~*i*~) to fix this and improve the model (as for this pair, the true relationship is quadratic). This constitutes a multiple linear regression problem.

2.  **Homoskedasticity** — *From the Scale-Location Plot*:

-   The red line is slightly upward-trending, indicating that variance increases with fitted values (minor heteroscedasticity)
-   The Tukey-Anscombe plot also seems to indicate that the scatter is not constant for the entire range of speed/fitted values (less scatter for lower values and more scatter for higher values). There is an obvious violation of homoskedasticity.
-   **Transformation**: Log-transform on dist (since stopping distance cannot be negative)

3.  **Independence**

-   Since the data is *not time-dependent*, residual independence is likely satisfied (no autocorrelation expected)
-   **Transformation**: None needed

4.  **Normality** — *From the Q-Q Plot*:

-   The bulk of the residuals (in the central region) are approximately Gaussian distributed. A noticeable deviations (or outliers) at the upper tail indicates right skewness hence departure from normality. The assumption of Gaussian errors is slightly violated by the model due to this moderate non-normality.
-   **Transformation**: Log-transform on dist to correct right-skewness (improve normality and heteroskedasticity)

#### **Model Evaluation and Improvements**

-   Therefore, this model (lm_s.sd = dist \~ speed) has minor assumption violations (non-linearity, heteroscedasticity, non-normality).

-   Suggested transformations like the log(dist) \~ speed and a quadratic term could be made and the diagnostics re-checked. The best model is the one with the most stable residuals, best-fulfilled assumptions, and highest adjusted R².

## Part 3: Simple regression

### Question 2

```{r, Part-3-2}
# Load the housing.rda data file
load(file.choose())
head(housing) # View first few rows of the dataset
summary(housing) # Get an overview of the dataset
str(housing)

# use regression analysis to explore the relationship between house size and price.
cat("\n A SLR between house price and house size \n")
lm_p.s <- lm(price ~ size, data = housing)
summary(lm_p.s)

cat("\n ===  SLR Model Plot  === \n")
plot(housing$size, housing$price) +
  abline(lm_p.s, col = "red")
```

#### (a)

**Comment on the Model Summary**

**Regression Coefficients**:

-   **Intercept** (133667.87): This means that for a theoretical house size of 0 units the predicted house price is 133667.87 units. This is not practically useful but ensures the regression line fits the data best within the observed size range. It is not meaningful to extrapolate to size = 0.
    -   It's *p-value* (\< 2e-16) is small and statistically significant at the 5% level, but its practical value is limited.
-   **Slope** (93.01): For every 1 unit increase in house size, the house price increases by about 93.01 units Bigger houses cost higher to buy.
    -   The *p-value* (0.0381) is smaller than 0.05 (even at a 1% significance level). We reject the null hypothesis at the 5% significance level. This means that house size has a statistically significant effect on house price, and we can be fairly confident (with 95% confidence) that the relationship isn't due to chance.

**Statistical Significance**:

-   From the F-statistic (4.417), the p-value for size is small (0.03814 \< 0.05), meaning the model is statistically significant at the 5% level.

**Model Goodness of Fit**:

-   The **multiple R-squared** (0.04313) and **adjusted R-squared** (0.03336) indicate that only 4.313% of the variability in house prices is explained by house size. This suggests the model is very weak in explanatory power and that other factors likely have a much bigger influence.

#### (b)

**Residual Diagnostics**

```{r, Part-3-2-b}
# Perform residual diagnostics and comment on model assumptions
cat("Performing Model Diagnostics \n")
cat("\n ===  Model Diagnostics Plots  === \n")
# Diagnostics plots
par(mfrow = c(2,2))
plot(lm_p.s)

par(mfrow = c(1,1))
# Tukey-Anscombe Plot
plot(lm_p.s$fitted.values, lm_p.s$residuals, xlab="Fitted", ylab="Residuals", pch=20) +
  title("Residuals vs. Fitted Values") +
  lines(loess.smooth(lm_p.s$fitted.values, lm_p.s$residuals),col="red") +
  abline(h=0, col="grey")

# Residuals vs. Predictor Plot
plot(housing$size, lm_p.s$residuals, xlab="predictor (size)", ylab="Residuals", pch=20) +
  title("Residuals vs. Predictor size") +
  lines(loess.smooth(housing$size, lm_p.s$residuals),col="red") +
  abline(h=0, col="grey")

# Quantile-Quantile Plot
qqnorm(lm_p.s$residuals) #Quantile-Quantile Plot
qqline(lm_p.s$residuals) # adds the diagonal line

# Evaluating Model Assumptions
cat("Model Assumption Evaluation \n")
```

**Comments on Model Assumptions**

1.  **Linearity** — *From the Tukey-Anscombe Plot (Residuals vs. Fitted)*:

-   From the plot, the residuals generally hover around the zero line which suggests that the E[E~*i*~] = 0 is approximately met. However, LOESS smoother line has a kink in the middle and largely deviates from the horizontal. The residuals for low and high house size (and respective fitted house price) values are systematically negative and they are positive for medium values. The linearity assumption is violated and a straight line is not the correct fit to the data. The model may be improved by variable transformation.

2.  **Homoskedasticity** — *From the Tukey-Anscombe plot and the Scale-Location Plot*:

-   The Tukey-Anscombe plot indicates a more or less constant scatter for the entire range of house size (& fitted) values. There is no obvious violation of homoskedasticity. The red line in the Scale-Location Plot is fairly horizontal which implies constant variance with fitted values (no heteroscedasticity).

3.  **Independence**

-   The residuals can be considered independent and uncorrelated.

4.  **Normality** — *From the Q-Q Plot*:

-   The bulk of the residuals (in the central region) lie on the 45^o^ line and thereby follow the Gaussian distribution. There are slight deviations (or outliers) at the lower and upper tail which indicate right skewness. The assumption of Gaussian errors is slightly violated by the model due to this moderate non-normality. Despite this, the approximation to normality in the center may be sufficient to validate this model.


#### (c)

**Check if a log transformation improves the model fit. Are any of the models useful?**

```{r, Part-3-2-c}
# Any right-skewness in the data?
# View distribution (histogram)
cat("\n Viewing Paramter Distributions: Check for skewness\n")
# Viewing House prices
hist(housing$price, freq = FALSE, breaks = 30, col = "lightblue",
     main = " Price Histogram with Density Curve", xlab = "Value", ylab = "Density",
     border = "black")

lines(density(housing$price, na.rm = TRUE), col = "red",lwd = 2) # Add density curve



# Adding a normal distribution curve for comparison
h_price <- seq(min(housing$price, na.rm = TRUE), max(housing$price, na.rm = TRUE), length.out = 100)
normal_price <- dnorm(h_price, mean = mean(housing$price, na.rm = TRUE), sd = sd(housing$price, na.rm = TRUE))
lines(h_price, normal_price, col = "blue", lwd = 2, lty = 2)

# Add legend
legend("topright", legend = c("Kernel Density", "Normal Distribution"), col = c("red", "blue"),
       lwd = 2, lty = c(1, 2))


# Viewing House Sizes
hist(housing$size, freq = FALSE, breaks = 30, col = "lightblue",
     main = " Size Histogram with Density Curve", xlab = "Value", ylab = "Density",
     border = "black")

lines(density(housing$size, na.rm = TRUE), col = "red",lwd = 2) # Add density curve

# Adding a normal distribution curve for comparison
h_size <- seq(min(housing$size, na.rm = TRUE), max(housing$size, na.rm = TRUE), length.out = 100)
normal_size <- dnorm(h_size, mean = mean(housing$size, na.rm = TRUE), sd = sd(housing$size, na.rm = TRUE))
lines(h_size, normal_size, col = "blue", lwd = 2, lty = 2)

# Add legend
legend("topright", legend = c("Kernel Density", "Normal Distribution"), col = c("red", "blue"),
       lwd = 2, lty = c(1, 2))

```

